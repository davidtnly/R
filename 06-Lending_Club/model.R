# Load scripts

# Load libraries
source("libraries.R") # frequently used libraries
source("functions.R") # frequently used functions

# Data
data <- read.csv("data.csv", stringsAsFactors = FALSE)

# Parallel
n.cores <- detectCores()    # Identify the machine's number of cores
registerDoParallel(n.cores) # Used to train model across multiple cores in parallel
n.cores <- getDoParWorkers()
paste(n.cores, 'workers utilized')

# Explore the data structure
details(data)

## Preprocessing and cleaning
checkna <- sort(colSums(sapply(data,is.na)), decreasing = TRUE) # unsure why it's not detecting values that is NA
# data <- data[,colnames(data) %in% colnames(data[colSums(is.na(data)) < 1000])] # remove any columns with over 1000 NAs

newcols <- c("loan_amnt","funded_amnt","funded_amnt_inv","term","int_rate","installment","grade","sub_grade",
             "emp_length","home_ownership","annual_inc","verification_status","issue_d","loan_status","pymnt_plan",
             "purpose","title","zip_code","delinq_2yrs","earliest_cr_line","inq_last_6mths","open_acc","pub_rec","revol_bal",
             "revol_util","total_acc","initial_list_status")
data <- data[,colnames(data) %in% newcols]
rm(newcols)

# Count uniques - remove any with high ratio indicating difficulty in detection
meta_data <- funModeling::df_status(data, print_results = FALSE)
head(meta_data,20)

# Create a new df for the unique table (remove any high unique ratios - too high of a variability to predict)
unique_df <- 
  meta_data %>%
  mutate(unique_ratio = unique / nrow(data))

unique_df %>% 
  select(variable, unique, unique_ratio) %>% 
  mutate(unique = unique, unique_ratio = scales::percent(unique_ratio)) %>% 
  kable()

# rm(meta_data,unique_df)

# Remove 4 NA loans without annual_inc
data <- data[!is.na(data$annual_inc),]

# Update variable data types (if any)
to_num <- c("loan_amnt","funded_amnt","funded_amnt_inv")

data <-
  data %>% 
  # mutate_at affects variables selected with a character vector
  # .funs = additional arguments for the function calls in .funs
  # .vars = a list of cols generated by vars()
  mutate_at(.funs = funs(as.numeric), .vars = to_num) # read up on .funs and .vars

to_date <- c("issue_d", "earliest_cr_line")

data %>%
  mutate_at(.funs = funs(convert_date), .vars = to_date)
  
# Get num vars
num_vars <-
  data %>% 
  sapply(is.numeric) %>% 
  which() %>% 
  names()

# Check to see if replacements are needed
meta_data %>% 
  select(variable, p_zeros, p_na, unique) %>% 
  filter(variable %in% num_vars) %>% 
  kable()

to_zero <- c("revol_util")

data %>%
  mutate_at(.funs = funs(replace(., is.na(.), 0)), .vars = to_zero)

# Define defaults and create new var
data %>% 
  group_by(loan_status) %>% 
  summarize(count = n(), pct = count/nrow(data)) %>% 
  knitr::kable()

default <- c("Default", "|Does not meet the credit policy. Status:Charged Off",
             "In Grace Period", "Late (16-30 days)", "Late (31-120 days)")

data <- data %>%
  mutate(defaulted = ifelse(!(loan_status %in% default), 0, 1))

data %>% 
  summarise(freq = sum(defaulted/n())) # about 2.4% of defaults

# Feature selection (select only a few first and include more in future EDA work)
features_used <- c("defaulted", "term", "loan_amnt","int_rate","grade","emp_length","annual_inc", "home_ownership")

loans <- data %>%
  select(one_of(features_used)) # one_of = select feature by names

# NA values for final data
sapply(loans , function(x) sum(is.na(x)))

# Exploratory Data Analysis
loans %>% 
  count(defaulted) %>% 
  ggplot(aes(x = reorder(defaulted, desc(n)), y = n, fill = n)) +
  geom_col() +
  coord_flip() +
  labs(x = "Status", y = "Count")

loans %>% 
  group_by(defaulted) %>% 
  summarise(freq = n()/nrow(loans)) %>% 
  ggplot(aes(x = reorder(defaulted, desc(freq)), y = freq, fill = c("#9e1904","#b7b726"))) +
  geom_col() +
  geom_text(aes(label = paste0(round(freq*100,1), "%"),
                size = 4, hjust = .1,
                fontface = "bold")) +
  coord_flip() +
  scale_y_continuous(labels = scales::percent) +
  # scale_colour_manual(values=c("#000000", "#FF5733")) +
  labs(x = "Status", y = "Frequency", title = "Lending Club's Delinquency Rate",
       caption = "1 = Default, 0 = Paid\nFrequency of delinquency loans in the dataset.\nCheck to see if we need to use resampling methods.") +
  theme(legend.position = "none",
        plot.title = element_text(hjust = 0.5))

# Plots to get a general idea on distribution between variables
options(scipen = 999999)

# for(i in 1:dim(loans)[2]) {
#   ggplot(data = loans, aes(x = loans[,i])) +
#     geom_histogram()
# }

ggplot(loans, aes(x = loans[,7])) +
  geom_histogram() # alot of loans have a corresponding annual income of zero

ggplot(loans[1:1000,], aes(x = loan_amnt, y = int_rate)) +
  geom_point()

# Assumptions that higher grade of loan would have lower default rate; check with plots
ggplot(loans, aes(x = grade, fill = as.factor(defaulted))) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  labs(x = "Loan Grade",
       y = "% of Default",
       fill = "Default",
       title = "Visualization of Defaults of Total Grades",
       caption = "As expected the higher grades have lower defaults visually") +
  # scale_color_manual(values = c("#999999","#E69F00")) +
  theme_economist() +
  theme(plot.title = element_text(hjust = 0.5))

# Count and find % - use spread() to find %
df <- loans %>% 
  group_by(grade,defaulted) %>% 
  summarise(ncount = n()) %>% 
  spread(key = defaulted, value = ncount) %>% # spread to make wider
  # could also use magrittr::set_colnames(c())
  `colnames<-` (c("grade", "good", "default")) %>% # change column names using `colnames<-` (c())
  mutate(default_rate = default/sum(good +default)) 

ggplot(data = df, aes(x = grade, y = default_rate, fill = default_rate)) +
  geom_bar(stat = "identity") +
  geom_text(aes(y = default_rate, label = paste0(round(default_rate*100,1),"%"), vjust = 1)) +
  labs(x = "Loan Grade",
       y = "Default Rate",
       fill = "Default %",
       title = "Default Rates by Grade",
       caption = "Visualization of loan defaults by grade: Best to Worst") +
  theme_dark() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.background = element_rect(fill = "grey"),
        legend.background = element_rect(fill = "grey"))

# Boxplot distribution looks fairly the same
# Although as we move from lower grades, there is a higher loan amount
# Based on boxplot outlier parameters, there are none besides in grade B
ggplot(data = loans, aes(x = grade, y = loan_amnt)) +
  geom_boxplot(fill = "white", colour = "blue", outlier.colour = "red", outlier.shape = 1) +
  facet_wrap(~ defaulted) +
  labs(x = "Grade",
       y = "Loan Amount",
       title = "Loan Amount by Grade") +
  theme_dark() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.background = element_rect(fill = "darkgrey"),
        axis.text = element_text(color = "white", face = "bold"),
        axis.ticks = element_blank()
  )

# Interest rate spreads with grade drop and definitely more outliers as well
# This might indicate that int rate may not matter as much as the grade
ggplot(data = loans, aes(x = grade, y = int_rate)) +
  geom_boxplot(fill = "white", colour = "blue", outlier.colour = "red", outlier.shape = 1) +
  facet_wrap(~ defaulted) +
  labs(x = "Grade",
       y = "Int Rate",
       title = "Interest Rate by Grade") +
  theme_dark() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.background = element_rect(fill = "darkgrey"),
        axis.text = element_text(color = "white", face = "bold"),
        axis.ticks = element_blank()
  )

# Use geom_point() for a portion of the data due to too many points
# Remove $600000 > annual_inc to see the plot better; questions on why high annual_inc would need a low loan_amnt
ggplot(loans[1:10000,][loans$annual_inc < 600000,], aes(x = annual_inc, y = loan_amnt)) +
  geom_jitter(aes(alpha = 0.1, color = "#00BA38")) +
  theme(legend.position = "none") +
  geom_smooth(method = "loess") + # plot loess fitted line to see where the trend moves; point x weighted towards nearest x
  labs(title = "Annual Income Vs. Loan Amount",
       x = "Income",
       y = "Loan") +
  theme_dark() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.background = element_rect(fil = "darkgrey"),
        axis.text = element_text(color = "white", face = "bold"),
        legend.position = "none")

# ggplot(loans, aes(x = grade, y = ..count.., fill = factor(defaulted, c("Fully Paid", "Defaulted")))) +
#   geom_bar() +
#   theme(legend.title = element_blank())

### Attempt to add in the year of the loan and split default rates by years to see if its inc or dec YoY

# Correlation; checking for multicollinearity
num_vars <- loans %>% 
  sapply(is.numeric) %>% 
  which() %>% 
  names()

all_num <- loans[, colnames(loans) %in% num_vars]

# Check for zero variance
nz_vars <- nearZeroVar(all_num, foreach = TRUE, saveMetrics = TRUE)
nz_all_num <- all_num[!nz_vars$nzv] # Remove any nzv that is TRUE
corrs <- cor(nz_all_num, use = "complete.obs")

# Played around with different variables of the plot although only 3 variables
par(xpd = TRUE) # shows var names
corrplot.mixed(corrs,
         # method = "color",
         # method = "number",
         lower = "number",
         upper = "color",
         mar = c(1,1,1,1),
         addgrid = TRUE,
         sig.level = 0.05,
         order = "hclust", # reorders
         tl.srt = 0,
         tl.col = "black",
         bg = "maroon"
         )
# Int and annual_inc negatively correlated
ggcorr(corrs,
       method = c("pairwise","spearman"),
       label_size = 10,
       label = TRUE
       )

# If there were many variables, use findCorrelation() and a cutoff value then remove variables
high_corr <- findCorrelation(corrs, names = TRUE, cutoff = 0.75) # set at 0.01 to populate some values
all_num <- all_num[,!colnames(all_num) %in% high_corr] # change all_num to loans

## Preprocess pre-Modeling
check_na(loans) # check for NA before splitting data
loans2 <- loans # testing
prep_data <- preProcess(loans2, method = c("center", "scale"))
df_transformed <- predict(prep_data, loans2)
model_var <- glm(as.factor(defaulted) ~ .,
                 data = df_transformed,
                 family = binomial(link = "logit"))

model_var
summary(model_var)

rm(all_num,df_transformed,prep_data,loans2,nz_all_num)

# ANOVA test on variables to see if there are high correlations
temp <- as.data.frame(anova(model_var)) # t-test for comparing means of more than two groups; this will see if there is high correlation
temp$feature = rownames(temp)
temp

## Modeling - Split Train/Test sets & Use a method for imbalanced issue
# Some popular models used for classification: logistic, RF, boosting, SVM
# If there were more variables we could use LASSO to reduce variables; ridge would be more appropriate here

# We have imbalanced data, which can cause bias, errors
# Ways to deal with imbalanced data
# Oversampling minority class, undersampling majority class, create new minority classes
# Definitely need to read more into how to best approach imbalanced data; (SMOTE, ROSE) methods are popular

# Try out a new function in caret::downSample
set.seed(100)
model_data <- downSample(x = loans[, !(names(loans) %in% c("defaulted"))],
                         y = as.factor(loans$defaulted), yname = "default")
base::prop.table(table(model_data$default)) # check proportion
base::table(model_data$default)

# Fix data types before modeling
model_data$term <- as.factor(model_data$term)
model_data$grade <- as.factor(model_data$grade)
model_data$emp_length <- as.factor(model_data$emp_length)
model_data$home_ownership <- as.factor(model_data$home_ownership)

# Split
set.seed(100)
inTrain <- createDataPartition(model_data$default, p = 0.75, list = FALSE)
train <- model_data[inTrain,]
test <- model_data[-inTrain,]
rm(model_data)

# Can either go straight into modeling or create matrices for 0 and 1 values per variable
# Hyperparameter setup
ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     number = 3)

glmnetGrid <- expand.grid(.alpha = seq(0, 1, length = 10), .lambda = 0.01)

## Models - usually I would start with basic steps in each model, tune, get best h-params for final models

# logistic
set.seed(100)
glm_model <- train(default ~ .,
                   data = train,
                   method = "glm",
                   # metric = "ROC", # testing for ROC would use this metric or "auc"
                   preProc = c("center", "scale"),
                   family = binomial(link = "logit"),
                   trControl = ctrl
                   )
# random forest
set.seed(100)
rf_model <- train(default ~.,
                  data = train,
                  method = "rf",
                  # metric = "ROC", # testing for ROC would use this metric or "auc"                  
                  trControl = ctrl,
                  preProc = c("center", "scale"),
                  grid = expand.grid(.mtry = c(1:4)), # expand.grid(.mtry = c(1:4), .ntree = c(100, 500, 1000))
                  ntrees = 100 # easier to tune until we figure out what's the best mtry
                  )
plot(rf_model)
summary(rf_model)

# Model Evals: confusionMatrix, ROC curve to test Accuracy, sensitivity (TP), specificity (TN)
glm_pred <- predict(glm_model, test, type = "response")
rf_pred <- predict(rf_model, test)

model_pred <- function(pred, t) {
  ifelse(pred > t, TRUE, FALSE)
}

confusionMatrix(data = model_pred(glm_pred, 0.5),
                reference = test$default,
                positive = TRUE)

confusionMatrix(data = model_pred(rf_model, 0.5),
                reference = test$default,
                positive = TRUE)

# Another model I like to use is XGB (pretty popular nowadays, although hard to interpret)
# one-hot-encoding
dtrain <- sparse.model.matrix(default ~ .-1, data = train)
dtest <- sparse.model.matrix(default ~ .-1, data = test)
# Check for same dim
dim(dtrain)
dim(dtest)

# Create keys
train_key <- train[,1]
test_key <- test[,1]

# Grid
xgb_grid <- expand.grid(nrounds = 100,
                        max_depth = c(1:4), # number of variables used
                        eta = c(0.01, 0.025, 0.05, 0.1), # learning rate(shrinkage); prevents overfitting
                        gamma = 1, # min loss reduction required to make further partition on a leaf node
                        colsample_bytree = c(.2, .4, .6, 1), # subsample ratio of columns for each tree
                        min_child_weight = 1, # tree partition step (larger the more conservative)
                        subsample = c(0.5, 0.75, 1) # setting 0.5 means would randomly sample half train data prior to growing trees preventing overfitting
                        )

# xgb
set.seed(100)
xgb_model <- train(y = factor(make.names(train_key)),
                   x = dtrain,
                   method = "xgbTree",
                   metric = "ROC",
                   tuneGrid = xgb_grid,
                   verbose = TRUE,
                   trControl = ctrl
                   )

# Plot tuning results
xgbTune
xgbTune$xgb_model
summary(xgb_model)

# Set best params
param <- list(objective   = 'binary:logistic',
              eval_metric = 'auc',
              max_depth   = xgbTune$bestTune$max_depth,
              eta         = xgbTune$bestTune$eta,
              gammma      = xgbTune$bestTune$gamma,
              colsample_bytree = xgbTune$bestTune$colsample_bytree, 
              min_child_weight = xgbTune$bestTune$min_child_weight,
              subsample = xgbTune$bestTune$subsample
)

set.seed(100)
xgb_fine_model <- xgboost(data = dtrain,
                          label = train_key,
                          nrounds = 500,
                          params = param,
                          verbose = 1,
                          trControl = ctrl
                          )
# Check performance
set.seed(100)
xgbfine_p <- predict(xgb_fine_model, dtest)
xgbfine_pred <- prediction(xgbfine_p, test_key)
xgbfine_perf <- performance(xgbfine_pred, "tpr", "fpr")
performance(xgbfine_pred, measure = "auc")@y.values

# Manual calc for xgb
best_thres <- function(predict,label) {
  k = 0
  accuracy = c()
  sensitivity = c()
  specificity = c()
  
  for(i in seq(from = 0.2 , to = 0.8 , by = 0.01)){
    k = k + 1
    preds_binomial = ifelse(predict > i , 1 , 0) # pred_loop
    confmat = table(label , preds_binomial)
    accuracy[k] = sum(diag(confmat)) / sum(confmat)
    sensitivity[k] = confmat[1 , 1] / sum(confmat[ , 1])
    specificity[k] = confmat[2 , 2] / sum(confmat[ , 2])
  }
  threshold <- seq(from = 0.20, to = 0.80, by = 0.01)
  threshold_df <- data.frame(threshold, accuracy, sensitivity, specificity)
  head(threshold_df)
  return(threshold_df)
}

best_cutoff <- function (df) {
  bst_cut <- max(df$accuracy)
  b1 <- df[df$accuracy == bst_cut,]
  t1 <- min(b1$threshold)
  t2 <- min(round(b1$accuracy,3))
  t3 <- min(round(b1$sensitivity,3))
  t4 <- min(round(b1$specificity,3))
  lst <- c(t1,t2,t3,t4)
  return(lst)
}

xgb_thres <- best_thres(xgbfine_p, test_label)
xgb_bc <- best_cutoff(xgb_thres)

# Plot
ggplot(gather(xgb_thres, key = "Metric", value = "Value", 2:4),
       aes(x = threshold, y = Value, color = Metric)) +
  geom_line(size = 1.5) +
  labs(caption = paste0("Best Threshold (Cutoff): ",xgb_bc[1],"\nAccuracy: ",xgb_bc[2],"\nSensitivity (TP): ",xgb_bc[3],"\nSpecitivity (TN): ",xgb_bc[4]),
       title = "XGB Test Threshold Cutoff Graph"
  ) +
  theme_economist() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.title = element_blank()) 

# Get feature names
names_train <- dimnames(dtrain)[[2]]
names_test <- dimnames(dtest)[[2]]

# Compute feature importance matrix
importance_matrix <- xgb.importance(names_train, model = xgb)[0:30]
importance_matrix_all <- data.frame(xgb.importance(names_train, model = xgb_fine_model))

# Plot features
xgb.plot.importance(importance_matrix)
